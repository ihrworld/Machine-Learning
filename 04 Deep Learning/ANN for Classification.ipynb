{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this study case is to create a job demographic segmentation model to tell the bank which of their customers are at highest risk of leaving.\n",
    "\n",
    "To achieve it, we are going to implement an Artificial Neural Network (ANN) Classification model using tensorflow and python to predict the movement of customers of the bank (whether or not they continue to be customers of the bank)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the tensorflow version that we are using\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Data preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s load the dataset and visualize the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the dataset\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           10000 non-null  int64  \n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(2), int64(9), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "#check the datatype of the dataset\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation about denpendent and independent variables...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get de independent variables and dependent variable\n",
    "X = dataset.iloc[:, 3:-1].values #we do not need the first 3 columns (info without impact on the dependent variable)\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 'Female' ... 1 1 101348.88]\n",
      " [608 'Spain' 'Female' ... 0 1 112542.58]\n",
      " [502 'France' 'Female' ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 'Female' ... 0 1 42085.58]\n",
      " [772 'Germany' 'Male' ... 1 0 92888.52]\n",
      " [792 'France' 'Female' ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Taking care of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if there are null values in the dataset\n",
    "dataset.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not have to take care of any missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A) Encoding the \"Gender\" column\n",
    "\n",
    "__Important:__ The column Gender has the index 2 in the dataset (index 0 is the column \"CreditScore\" and the index 1 is the column called \"Geography\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "X[:, 2] = le.fit_transform(X[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 0 ... 1 1 101348.88]\n",
      " [608 'Spain' 0 ... 0 1 112542.58]\n",
      " [502 'France' 0 ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 0 ... 0 1 42085.58]\n",
      " [772 'Germany' 1 ... 1 0 92888.52]\n",
      " [792 'France' 0 ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B) One Hot Encoding the \"Geography\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding the Geography column creating dummy variables\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 ... 1 1 101348.88]\n",
      " [0.0 0.0 1.0 ... 0 1 112542.58]\n",
      " [1.0 0.0 0.0 ... 1 0 113931.57]\n",
      " ...\n",
      " [1.0 0.0 0.0 ... 0 1 42085.58]\n",
      " [0.0 1.0 0.0 ... 1 0 92888.52]\n",
      " [1.0 0.0 0.0 ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the training set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4  Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Feature Scaling is absolutely compulsory for deep learning and categorization.__\n",
    "\n",
    "So, we have to apply always feature scaling in all the variables of our dataset when we are working with Artificial Neural Networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Building the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Initializing the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build an ANN, firstly we have to create a new object that will be the properly ANN itself. This new object belongs to the sequential class, as an ANN is actually a sequence of layers, which starts from the input layer and then we have hidden layers fully connected until the output layer.\n",
    "\n",
    "In addition, the sequential class comes from the Keras module of TensorFlow 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the object for the ANN\n",
    "ann = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Adding the input layer and the first hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to add a fully connected layer into an ANN at whatever phase you are is using the “dense class”. So, we have to take our object and call the “add” method of the sequential class. The layers will be created as objects of a new class, which is the “dense class”. Regarding the hidden layer, we will choose the default option of 6 hidden neurons. \n",
    "\n",
    "__Important:__ We do not have to enter the number of features that we want for the input layer because the features will be recognized automatically by tensorflow. So, once we included the matrix of features in the training, the ANN will automatically collect these four features. Therefore, we do not need to specify that we have 4 features.\n",
    "\n",
    "On another hand, we will use the “rectifier” function for the activation function parameter, which will break the linearity of the operations happening between this input layer and the first hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a simple connected layer\n",
    "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Adding the second hidden layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second hidden layer, we will use the same code as the first hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the second hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Adding the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to build output layer fully connected to that second hidden layer and we need to use the “dense class” again.\n",
    "\n",
    "We want to predict a binary variable [0, 1]. So, it is enough to take only one neuron. \n",
    "\n",
    "On another hand, as we are in the output layer, we have to replace the “rectifier activation function” for the “sigmoid activation function”. What will we get?\n",
    "* Get the predictions of whether the customers choose to leave or not the bank.\n",
    "* We have for each customer the probability that the customers leave the bank.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the output layer\n",
    "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Training the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Compiling the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the “compile” method to compile the ANN. Here we have to include three parameters:\n",
    "* __Optimizer__ =>> choose an optimizer to adjust the weights through stochastic gradient descent and reduce the loss function in the next iteraction. The most common is the “adam” optimizer.\n",
    "* __Loss function__ =>> It computes the difference between the predictions and the real result. For binary classification, we can use the “binary_crossentropy” loss function. For non-binary classification, we use the “categorical_crossentropy” loss function when we are predicting more than two categories.\n",
    "* __Matrix__ =>> “accuray”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling the ANN\n",
    "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training the ANN on the Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to use the “fit” method to train out ANN. Here we have to enter two main parameters:\n",
    "\n",
    "* __Number of epochs__ => Forward-propagation and Backward-propagation happens over many epochs and over each epoch the loss functions is slightly reduced. Therefore, we want to repeat these epochs in order to reduce more little by little the loss function. By default, we can use 100 epochs.\n",
    "* __Batch size__ => Instead of propagating all the features one by one, we propagate them in batches of a certain number of elements of a certain sets of the features. By default, we can use the 32 in the batch size.\n",
    "\n",
    "When we apply backward-propagation, we can adjust the weights of the connections between the neurons. With this action, the loss function approaches 0 the next time that we will use the ANN for a prediction. \n",
    "\n",
    "In order to get it, we need to use the “gradient descent”. It will change the weights in small increments with the calculation of the derivate (gradient) of the loss function, which allow us to see the descent direction until the global minimum. This calculation is done in batches  in the following interactions (epochs) of the data that we are sending along the ANN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 1s 123us/sample - loss: 0.4953 - accuracy: 0.7960\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 0s 39us/sample - loss: 0.4560 - accuracy: 0.7960\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 0s 40us/sample - loss: 0.4438 - accuracy: 0.7960\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 0s 48us/sample - loss: 0.4388 - accuracy: 0.7960\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 0s 45us/sample - loss: 0.4356 - accuracy: 0.7960\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 0s 42us/sample - loss: 0.4335 - accuracy: 0.7960\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 0s 40us/sample - loss: 0.4320 - accuracy: 0.7960\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 0s 40us/sample - loss: 0.4305 - accuracy: 0.7960\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 0s 41us/sample - loss: 0.4294 - accuracy: 0.7960\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 0s 43us/sample - loss: 0.4282 - accuracy: 0.7960\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 0s 48us/sample - loss: 0.4268 - accuracy: 0.8087\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 0s 40us/sample - loss: 0.4256 - accuracy: 0.8123\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 0s 39us/sample - loss: 0.4243 - accuracy: 0.8139\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 0s 42us/sample - loss: 0.4232 - accuracy: 0.8161\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 0s 40us/sample - loss: 0.4217 - accuracy: 0.8176\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 0s 39us/sample - loss: 0.4197 - accuracy: 0.8209\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 0s 42us/sample - loss: 0.4170 - accuracy: 0.8232\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 0s 47us/sample - loss: 0.4141 - accuracy: 0.8266\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 0s 44us/sample - loss: 0.4111 - accuracy: 0.8311\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 0s 44us/sample - loss: 0.4081 - accuracy: 0.8329\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 0s 38us/sample - loss: 0.4060 - accuracy: 0.8347\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.4043 - accuracy: 0.8360\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 0s 38us/sample - loss: 0.4030 - accuracy: 0.8366\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 0s 39us/sample - loss: 0.4019 - accuracy: 0.8361\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.4009 - accuracy: 0.8366\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 0s 38us/sample - loss: 0.4000 - accuracy: 0.8367\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 0s 39us/sample - loss: 0.3989 - accuracy: 0.8364\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 0s 39us/sample - loss: 0.3983 - accuracy: 0.8361\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 0s 43us/sample - loss: 0.3975 - accuracy: 0.8364\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 0s 40us/sample - loss: 0.3969 - accuracy: 0.8371\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 0s 38us/sample - loss: 0.3967 - accuracy: 0.8386\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.3961 - accuracy: 0.8374\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 0s 40us/sample - loss: 0.3956 - accuracy: 0.8379\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.3951 - accuracy: 0.8382\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.3949 - accuracy: 0.8376\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 0s 40us/sample - loss: 0.3949 - accuracy: 0.8372\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.3945 - accuracy: 0.8395\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 0s 38us/sample - loss: 0.3944 - accuracy: 0.8375\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 0s 38us/sample - loss: 0.3944 - accuracy: 0.8374\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 0s 39us/sample - loss: 0.3936 - accuracy: 0.8390\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 0s 38us/sample - loss: 0.3940 - accuracy: 0.8388\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.3936 - accuracy: 0.8384\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 0s 40us/sample - loss: 0.3934 - accuracy: 0.8372\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.3934 - accuracy: 0.8374\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 0s 38us/sample - loss: 0.3934 - accuracy: 0.8376\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 0s 42us/sample - loss: 0.3930 - accuracy: 0.8376\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 0s 38us/sample - loss: 0.3930 - accuracy: 0.8378\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 0s 38us/sample - loss: 0.3926 - accuracy: 0.8380\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 0s 40us/sample - loss: 0.3928 - accuracy: 0.8369\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 0s 38us/sample - loss: 0.3926 - accuracy: 0.8370\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 0s 38us/sample - loss: 0.3925 - accuracy: 0.8372\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 0s 38us/sample - loss: 0.3924 - accuracy: 0.8372\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 0s 41us/sample - loss: 0.3923 - accuracy: 0.8379\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 0s 39us/sample - loss: 0.3924 - accuracy: 0.8376\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.3922 - accuracy: 0.8372\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 0s 38us/sample - loss: 0.3920 - accuracy: 0.8379\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.3920 - accuracy: 0.8382\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.3921 - accuracy: 0.8380\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 0s 40us/sample - loss: 0.3919 - accuracy: 0.8374\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.3919 - accuracy: 0.8376\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 0s 38us/sample - loss: 0.3916 - accuracy: 0.8382\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 0s 39us/sample - loss: 0.3918 - accuracy: 0.8379\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 0s 41us/sample - loss: 0.3913 - accuracy: 0.8380\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 0s 36us/sample - loss: 0.3916 - accuracy: 0.8389\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 0s 40us/sample - loss: 0.3914 - accuracy: 0.8378\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 0s 41us/sample - loss: 0.3914 - accuracy: 0.8374\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.3911 - accuracy: 0.8388\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.3910 - accuracy: 0.8375\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 0s 39us/sample - loss: 0.3911 - accuracy: 0.8380\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.3912 - accuracy: 0.8386\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.3910 - accuracy: 0.8372\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 0s 39us/sample - loss: 0.3909 - accuracy: 0.8386\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 0s 36us/sample - loss: 0.3910 - accuracy: 0.8384\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.3910 - accuracy: 0.8374\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 0s 39us/sample - loss: 0.3905 - accuracy: 0.8385\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.3907 - accuracy: 0.8382\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.3905 - accuracy: 0.8376\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.3903 - accuracy: 0.8386\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 0s 39us/sample - loss: 0.3900 - accuracy: 0.8386\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.3901 - accuracy: 0.8381\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 0s 51us/sample - loss: 0.3897 - accuracy: 0.8393\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 0s 45us/sample - loss: 0.3900 - accuracy: 0.8389\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 0s 42us/sample - loss: 0.3897 - accuracy: 0.8399\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 0s 60us/sample - loss: 0.3895 - accuracy: 0.8391\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 0s 52us/sample - loss: 0.3895 - accuracy: 0.8393\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 0s 40us/sample - loss: 0.3895 - accuracy: 0.8396\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 0s 49us/sample - loss: 0.3891 - accuracy: 0.8406\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 0s 44us/sample - loss: 0.3891 - accuracy: 0.8393\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 0s 44us/sample - loss: 0.3892 - accuracy: 0.8396\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 0s 42us/sample - loss: 0.3890 - accuracy: 0.8381\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 0s 42us/sample - loss: 0.3889 - accuracy: 0.8401\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 0s 44us/sample - loss: 0.3890 - accuracy: 0.8397\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 0s 44us/sample - loss: 0.3885 - accuracy: 0.8396\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 0s 41us/sample - loss: 0.3885 - accuracy: 0.8394\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 0s 59us/sample - loss: 0.3885 - accuracy: 0.8396\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 0s 47us/sample - loss: 0.3883 - accuracy: 0.8393\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 0s 40us/sample - loss: 0.3882 - accuracy: 0.8410\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 0s 40us/sample - loss: 0.3883 - accuracy: 0.8390\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 0s 54us/sample - loss: 0.3879 - accuracy: 0.8411\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 0s 39us/sample - loss: 0.3879 - accuracy: 0.8405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24eb5b64948>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training the ANN\n",
    "ann.fit(X_train, y_train, batch_size = 32, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Making the predictions and evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Predicting the result of a single observation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to try to predict the outcome of a single observation, meaning a single customer.\n",
    "\n",
    "In order to do it, please, use our ANN model to predict if the customer with the following informations will leave the bank:\n",
    "\n",
    "Geography: France\n",
    "\n",
    "Credit Score: 600\n",
    "\n",
    "Gender: Male\n",
    "\n",
    "Age: 40 years old\n",
    "\n",
    "Tenure: 3 years\n",
    "\n",
    "Balance: $ 60000\n",
    "\n",
    "Number of Products: 2\n",
    "\n",
    "Does this customer have a credit card? Yes\n",
    "\n",
    "Is this customer an Active Member: Yes\n",
    "\n",
    "Estimated Salary: $ 50000\n",
    "\n",
    "Question: Should we say goodbye to that customer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12957944]]\n"
     ]
    }
   ],
   "source": [
    "#predicting the result of a single observation\n",
    "#predicted probability\n",
    "print(ann.predict(sc.transform([[1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False]]\n"
     ]
    }
   ],
   "source": [
    "#predicting the result of a single observation\n",
    "#predicted absoluted result\n",
    "print(ann.predict(sc.transform([[1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])) > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution =>> Our ANN model predicts that this customer stays in the bank!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Predicting the test results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s check how the model predict the test results.\n",
    "\n",
    "We have to use the “predict” method in the test set. Then, we will get all these predictions of the test in a new vector, which is called the y_pred. Finally, we can compare the y_pred with the real results (y_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 1]\n",
      " [0 0]\n",
      " ...\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "#predicting the absolute results of the test set\n",
    "y_pred = ann.predict(X_test)\n",
    "y_pred = (y_pred > 0.5) #show the results in a non-probability form\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>>  The results look really good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Building the Confussion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real way to check the performance of the models is building a confussion matrix and check the accuracy of the ANN in the test set.\n",
    "\n",
    "So, let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1541   54]\n",
      " [ 258  147]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.844"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 84% of the values were predicted correctly. \n",
    "\n",
    "So, the model is not properly perfect, but it can predict correctly a huge amount of data, and it is a good result!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
